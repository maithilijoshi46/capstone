{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering analysis using PCA\n",
    "\n",
    "\n",
    "What you will find in here:\n",
    "- [Clustering Analysis](#Clustering)\n",
    "- [Variance](#Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all my imports necessary \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, f1_score\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products_train= pd.read_csv(\"../data/order_products__train.csv\")\n",
    "order_products_prior = pd.read_csv(\"../data/order_products__prior.csv\")\n",
    "orders = pd.read_csv(\"../data/orders.csv\")\n",
    "products = pd.read_csv(\"../data/products.csv\")\n",
    "aisles = pd.read_csv(\"../data/aisles.csv\")\n",
    "departments = pd.read_csv(\"../data/departments.csv\")\n",
    "orders_train_all = pd.read_csv(\"../data/orders_train_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly selecting some of the data\n",
    "\n",
    "PCA wouldn't run with the number of rows and columns in the original dataframe, so I created a smaller subset from the dataframe I created with all my engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsetting from the prior eval set\n",
    "priors = orders.loc[orders['eval_set']=='prior']\n",
    "\n",
    "#looking at unique user ID's\n",
    "prior_users = priors['user_id'].unique().tolist()\n",
    "\n",
    "#gathering a smaller, random sample from the original set \n",
    "#in order to conduct a PCA and have it not take 10 million years\n",
    "random_prior = random.sample(range(len(prior_users)), 25000)\n",
    "\n",
    "\n",
    "#new users and the random subset mapped onto each other\n",
    "users_prior = [prior_users[i] for i in random_prior]\n",
    "\n",
    "#iterating through user id and making sure it's in the new subset dataframe\n",
    "sample_orders = orders[orders['user_id'].isin(random_prior)]\n",
    "\n",
    "#creating a list of orders within this new dataframe\n",
    "orders_list = sample_orders['order_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "priors_sample = orders_train_all[orders_train_all[\"order_id\"].isin(orders_list)]\n",
    "priors_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors_sample.to_csv('../data/priors_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Clustering'></a>\n",
    "# Now to the clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['product_id', 'aisle_id', 'department_id', 'order_id',\n",
    "       'add_to_cart_order', 'user_id', 'order_number',\n",
    "       'order_dow', 'order_hour_of_day', 'days_since_prior_order',\n",
    "       'average_orders', 'average_order_dow', 'average_order_hour', 'weekend',\n",
    "       'rate_reordered', 'aisle_cat', 'dept_cat']\n",
    "\n",
    "#Features\n",
    "X = priors_sample[features]\n",
    "y = priors_sample.reordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                 random_state = 42,\n",
    "                 stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate and fit a PCA model\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_sc)\n",
    "X_test_pca = pca.transform(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place PCA into XGBoost\n",
    "xg = XGBClassifier(max_depth = 12, \n",
    "                  min_child_weight= 3,\n",
    "                   random_state=42)\n",
    "\n",
    "xg.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = xg.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 Score: %.2f%%\" % round((f1_score(y_test, y_pred)* 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Variance'></a>\n",
    "# Explained Variance\n",
    "\n",
    "- 83.3% of our variance can be explained by 10 of our variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_exp = pca.explained_variance_ratio_\n",
    "print(f'Explained variance: {np.round(var_exp,3)}')\n",
    "\n",
    "#cumulative variance explained\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(f'Cumulative explained variance: {np.round(cum_var_exp,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it turns out that clustering doesn't do much better than XGBoost alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lesson 8.4\n",
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "component_number = range(len(cum_var_exp))\n",
    "plt.plot(component_number, cum_var_exp, lw=3)\n",
    "\n",
    "# Add horizontal lines at y=0 and y=100\n",
    "plt.axhline(y=0, linewidth=1, color='grey', ls='dashed')\n",
    "plt.axhline(y=1, linewidth=1, color='grey', ls='dashed')\n",
    "\n",
    "# Set the x and y axis limits\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([-1,26])\n",
    "ax.set_ylim([-0.05,1.05])\n",
    "\n",
    "# Label the axes\n",
    "ax.set_ylabel('cumulative variance explained', fontsize=16)\n",
    "ax.set_xlabel('component', fontsize=16)\n",
    "\n",
    "# Make the tick labels bigger\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "    \n",
    "# Add title\n",
    "ax.set_title('Component vs Cumulative variance explained\\n', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
